\documentclass[../main.tex]{subfiles}
\begin{document}

% While the ambiguity decomposition has already been published in \citeyear{krogh_NeuralNetworkEnsembles_1995}, the decomposition into average bias, average variance and expected ambiguity is very recent \cite{wood_UnifiedTheoryDiversity_2023}. 
% Numerous other measures of ensemble "diversity" have been developed. In this section, we will review some of these measures with a focus on binary classification. 

For a suitable diversity measure, the following properties are desirable.
\begin{itemize}
  \item The measure captures the differences between member models.
  \item There is a relationship between the diversity measure and the ensemble generalisation error.
  \item The diversity measure is independent of the outcome variable.
\end{itemize}

Naturally, many diversity measures are based on some notion of spread of the member models. This can, for instance, be a measure of disagreement, variance, impurity, entropy, or covariance.

\subsection{Disagreement}
\begin{margintable}
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
         & $q_i = +1$ & $q_i = -1$ \\ \hline
        $q_j = +1$ & $n_{(+,+)}$ & $n_{(+,-)}$ \\ \hline
        $q_j = -1$ & $n_{(-,+)}$ & $n_{(-,-)}$ \\ \hline
    \end{tabular}
    \caption{Notation for entries of the contingency table.}
    \label{tab:nplusminus}
\end{margintable}
We will begin with two simple measures based on the contingency table \cite{zhou_EnsembleMethodsFoundations_2012}. To shorten notation, we will refer to its entries as given in table \ref{tab:nplusminus}.
The \textit{Disagreement Measure} is the proportion of examples on which two members make different predictions.
$$
\frac{1}{n} (n_{(+,-)} + n_{(-,+)})
$$
The \textit{$Q$-Statistic} is given as
$$
Q_{ij}\defeq \frac{n_{(+,+)} n_{(-,-)} - n_{(-,+)} n_{(+,-)}}{n_{(+,+)} n_{(-,-)} + n_{(-,+)} n_{(+,-)}}
$$
$Q_{ij} = 0$ if $q_{i}$ and $q_{j}$ are independent, positive if the members make similar predictions and negative if the members make different predictions.

\subsection{Ambiguity}

\cite{krogh_NeuralNetworkEnsembles_1995} propose a decomposition of the ensemble generalisation error into two terms: One describing the average error of ensemble members and a so-called \textit{ambiguity} term. 
% TODO wrap this in theorem
$$
\mathbb{E}_{}\left[ \ell(y, \bar{q}) \right] = \Mavg \ell(y, q_{i}) - \Mavg \ell(\bar{q}, q_{i})
$$
This perfectly divides the ensemble error into error due to characteristics of the individual member models and error due to interactions between member predictions. Note that the contribution of the ambiguity term here is negative, that is, ambiguity is a beneficial influence. 
This decomposition was originally given for the squared-error loss \cite{krogh_NeuralNetworkEnsembles_1995} and the KL-divergence \cite{todo}. We will treat this quantity in detail in the rest of this chapter. Right now, note that the ambiguity term can be interpreted as a measure of \textit{variance} where individual distances are measured by the loss $\ell$. For the squared-error loss and the arithmetic mean combiner, this is exactly the "statistical" variance over the members $\Mavg \left(q_{i} - \Mavg q_{i}\right)^2$. For other loss functions, this yields other well-known quantities (see definition \ref{def:bregman-information}).

\subsection{Impurity} 
\citeauthor{KohaviWolpert} give a bias-variance decomposition of the \zeroone-loss as follows.
$$
\begin{align}
\mathbb{E}_{}\left[ \Lzo{Y}{q(X)} \right] &= 
\mathbb{E}_{}\left[  
\nicefrac{1}{2}\left( \sigma^2_{x} + \text{bias}(q) + \text{var}(q)   \right)
\right] \\
\text{for} ~ ~ & \text{bias}(q) =  \sum_{y} \left(  \prob{}{y^\star = y ~|~x} - \prob{}{q=y ~|~ x} \right)^2 \\
& \text{var}(q) = 1 - \sum_{y} \prob{}{q=y ~|~x}^2  \\
& \sigma^2 = 1 - \sum_{y} \prob{}{y^\star = y ~|~ x}
\end{align}
$$
Note that the variance and noise terms are of the form of the Gini impurity (see \ref{sec:gini-impurity}) and can be interpreted as measures of impurity.
\citeauthor{kuncheva_MeasuresDiversityClassifier_2003} take inspiration from this variance term and proceed as follows. Instead of considering the impurity over positive or negative labels, they instead consider the impurity over labels for which the ensemble is correct ($\tilde{y} = +1$) or incorrect ($\tilde{y} = -1$), respectively. As such, the probability is not with respect to the distribution of labels but the distribution $\Theta$ of members in the ensemble. Let the number of correct classifiers on an example be $m_{+}(x) \defeq \sum_{j}^M \ind{q_{j}(x) = y(x)}$ where $y(x)$ is the true label of $x$. This yields
$$
\text{var}'_{x} \defeq 1 - \sum_{\tilde{y}} \prob{\Theta}{q = \tilde{y} ~ |~ x}^2
$$
Averaging this over the entire training dataset, this can be understood as a measure of diversity. More precisely, it is the impurity as measured by the Gini impurity of the predictions of the ensemble members.
% TODO ref to def to gini imp
% TODO I feel like this is related to diversity-effect for 0/1 via ensemble margins (all the 2 times, and 1-)
% TODO can this be related to good/bad diversity?

\subsection{Entropy}
% TODO what is diversity term for KL-divergence? entropy? or rather, mutual information? (see section on bregman information)
\citeauthor{cunningham_DiversityQualityClassification_2000} propose the entropy between member predictions as a diversity measure. For a single point, it is given as
$$
\sum_{y \in \{ -1, +1 \}} - \prob{\Theta}{y ~|~x} \log \prob{\Theta}{y~|~x}
$$
where, for an ensemble, $\prob{\Theta}{y~|~x} \approx \Mavg \ind{q_{i}(x) = y}$.
\citeauthor{shipp_RelationshipsCombinationMethods_2002} propose a target-dependent measure that is reminiscent of the entropy. With $m_{+}(x) = \sum_{j=1}^M \ind{q_{i}(x) = y(x)}$:
$$
\frac{1}{M - \ceil{\frac{M}{2}}} \min \{ m_{+}(x), M-m_{+}(x) \}
$$

\subsection{Covariance}
\label{sec:bias-variance-covariance}

\paragraph{Covariance of members}
In section \ref{sec:ensemble-learning-motivation}, we have already seen evidence that the covariance between members is an essential factor to ensemble performance. Indeed, the notion of covariance and uncorrelatedness has been a guiding thought in the literature \cite{didaci_DiversityClassifierEnsembles_2013, brown_ManagingDiversityRegression_2005, buschjager_GeneralizedNegativeCorrelation_2020}.
We will now introduce an exact decomposition of the ensemble error for the squared-error loss that includes the average covariance between member predictions.
\begin{theorem} (Bias-Variance-Covariance decomposition \ref{ueda_GeneralizationErrorEnsemble_1996, brown_ManagingDiversityRegression_2005})
  \label{thm:bias-variance-covariance}
It holds that
$$
\begin{align}
\mathbb{E}_{(X,Y),D}\left[ (y - \bar{q})^2 \right]  &= \overtext{bias}^2 + \frac{1}{M} \overtext{var} + \left( 1 - \frac{1}{M} \right) \overtext{covar} \\ \\ 
\text{for} ~ ~ & \overtext{bias} =_{\text{def}} \Mavg (\mathbb{E}_{D}\left[ q_{i}  \right] - y) \\
& \overtext{var} =_{\text{def}} \Mavg \mathbb{E}_{D}\left[ (q_{i} - \mathbb{E}_{D}\left[ q_{i} \right])^2  \right]  \\
& \overtext{covar} =_{\text{def}} \frac{1}{M(M-1)} \sum_{i \not= j} \mathbb{E}_{D}\left[ (q_{i} - \mathbb{E}_{D}\left[ q_{i} \right] ) (q_{j} - \mathbb{E}_{D}\left[ q_{j} \right] ) \right] 
\end{align}
$$
\end{theorem}
\begin{proof}
We begin by applying the bias-variance decomposition (theorem  \ref{thm:bias-variance-effect}) to the ensemble model $\bar{q}$.
\begin{align*}
\mathbb{E}_{}\left[ \ell(y, \bar{q})  \right]  
&= \mathbb{E}_{}\left[ \ell(Y, y^\star) \right]  + \mathbb{E}_{}\left[ \ell(\bar{q}^\star, y^\star) \right] + \mathbb{E}_{}\left[ \ell(\bar{q}^\star, \bar{q}) \right]  \\
\end{align*}
For the bias term, it holds that
\begin{align*}
\text{bias} (\bar{q}) = 
\ell(\bar{q}^\star, y^\star)
&=  (\mathbb{E}_{D}\left[ \bar{q} \right]  - y^\star)  \\
&=  \mathbb{E}_{D}\left[ \left(\left(\Mavg q_{i}\right) - y^\star\right)^2 \right]   \\
&=  \left(\Mavg \mathbb{E}_{D}\left[ q_{i} - y^\star \right] \right)^2   \\
&=  \overline{\text{bias} }^2   
\end{align*}
The variance of the ensemble can be decomposed into terms describing variances of individual members and covariances between members.
\sidenote{
Using that
$$
\left( \sum_{i=1}^n a_{i}\right)^2 = \sum_{i=1}^n a_{i} + \sum_{j \not= i} a_{i} a_{j}
$$
}
\begin{align*}
\Var{}{\bar{q}} 
&= \mathbb{E}_{D}\left[ \ell(\bar{q}, \bar{q}^\star) \right] \\
&= \mathbb{E}_{D}\left[ (\bar{q} - \mathbb{E}_{D}\left[ \bar{q} \right] )^2 \right]  \\
&= \mathbb{E}_{D}\left[ \left(  \Mavg q_{i} - \mathbb{E}_{D}\left[ \Mavg q_{i} \right]  \right)^2 \right] \\
&= \frac{1}{M^2} \sum_{i=1}^M \mathbb{E}_{D}\left[ (q_{i} - \mathbb{E}_{_{D}}\left[ q_{i} \right] )^2 \right] 
+ \frac{1}{M^2} \sum_{j \not=i} \mathbb{E}_{D}\left[ (q_{i} - \mathbb{E}_{D}\left[ q_{i} \right] ) (q_{j} - \mathbb{E}_{D}\left[ q_{j} \right] ) \right] 
\end{align*}
Rearraging the coefficients yields the form of the theorem.
\sidenote{
The coefficients are rearranged using
$$
\frac{1}{M^2} = \left( 1-\frac{1}{M} \right) \frac{1}{M(M-1)}
$$
}
\end{proof}
The Bias-Variance-Covariance decomposition can be interpreted as a decomposition into characteristics of individual learners (mean bias and variance), plus a quantity describing the interactions between diferent learners. Again, one can see that ensemble performance profits if members are uncorrelated.
However, this decomposition is only given for the squared error and the arithmetic mean combiner. 


% TODO maybe reintroduce(would have to fix notiation)
% but really don't see why we would need that
% \paragraph{A covariance decomposition for classification}
% % TODO not sure whether this should be included
% \citeauthor{didaci_DiversityClassifierEnsembles_2013} builds on the decomposition given by \citeauthor{kohavi_BiasVarianceDecomposition_} and, similar to the derivation of the covariance decomposition for the squared-error loss, derive average bias, average variance and covariance terms.  They decompose the bias and variance terms of the ensemble as follows, writing $P\left[y_i\right]\defeq P\left[Y=y_i \mid \mathbf{x}\right]$ and $\hat{P}_{q}\left[y_i\right] \defeq P[q=\left.y_i\right]$. 
% % TODO use PP
% \begin{align*}
% \operatorname{bias}(\bar{q}) & =\nicefrac{1}{2} \sum_{y_i}\left(P\left[y_i\right]-\frac{1}{\sqrt{N}} \sum_j \hat{P}_{q_{j}}\left[y_i\right]+\frac{1}{\sqrt{N}} \sum_j \hat{P}_{q_{j}}\left[y_i\right]-\hat{P}_{\bar{q}}\left[y_i\right]\right)^2 \\
% & =\overline{\operatorname{bias}}+b, \\ \\
% \operatorname{var}(\bar{q}) & =\nicefrac{1}{2}\left(1-\frac{1}{N^2} \sum_{j, y_i} \hat{P}_{q_{j}}^2\left[y_i\right]+\frac{1}{N^2} \sum_{j, y_i} \hat{P}_{q_{j}}^2\left[y_i\right]-\sum_{y_i} \hat{P}_{\bar{q}}\left[y_i\right]^2\right) \\
% & =\frac{1}{N} \overline{\operatorname{var}}+v,
% \end{align*}
% However, there are remaining terms $b$ and $v$ and, as the authors state, their interpretation is yet unclear.


\subsection{Relationship between Ambiguity and Covariance}

Ambiguity as a generalised notion of variance, and the well-known covariance decomposition.  Can see that strictly speaking, these belong to the "variance" section.

We have now seen two approaches to expressing the ensemble generalisation error:
\begin{itemize}
\item In terms of \textit{covariance} as in the bias-variance-covariance decomposition of theorem \ref{thm:bias-variance-covariance}. A very similar notion also appears in when considering ensemble margins as in \ref{thm:breiman}
\item In terms of variation around a centroid with respect to some loss function as in the diversity decomposition of theorems \ref{thm:bias-variance-diversity-effect-decomp}. For the squared-error loss, this is exactly the variance. The generalisations to Bregman divergences and arbitrary loss functions can be understood as generalised measures of variance. 
\sidenote{
For any loss function:
$$
\mathbb{E}_{D}\left[ \LE{q^\star}{q} \right] 
$$
For the squared-error loss:
$$
\mathbb{E}_{D}\left[ (q - \mathbb{E}_{D}\left[ q \right] )^2 \right] = \mathbb{E}_{D}\left[ \ell(q^\star, q) \right] 
$$
For Bregman divergences, this is analogous to the \textit{Bregman information}, see definition \ref{def:bregman-information}:
$$
\mathbb{E}_{D}\left[ \Breg{q^\star}{q} \right] 
$$
}
\end{itemize}
Inspecting the proofs of theorems \ref{thm:bias-variance-covariance} and \ref{thm:breiman}, one can observe that in both cases, one begins with a measure of variance. Based on this, a covariance expression is then extracted, enabled by the definition of variance involving a square, producing cross-terms.
In other words, \textit{the covariance term is an artifact of the squared-error loss}. The notion of variance is more general.
% TODO corollary: uncorrelatedness is a special case of diversity

For the case of squared-error loss, \citeauthor{brown_ManagingDiversityRegression_2005} \cite{brown_ManagingDiversityRegression_2005} relate the ambiguity decomposition (\ref{thm:ambig-effect-decomp}) and the bias-variance-covariance decomposition (theorem \ref{thm:bias-variance-covariance}) directly. 
\begin{align*}
\mathbb{E}_{}\left[ \Mavg (y - q_{i})^2 - \Mavg (\bar{q} - q_{i})^2 \right]  = \overline{\text{bias} } + \frac{1}{M} \overline{\text{var} } + \left( 1 - \frac{1}{M} \right) \overline{\text{covar} }
\end{align*}
which yields
\begin{align*}
\mathbb{E}_{}\left[ \Mavg (y - q_{i})^2 \right]  &= \overline{\text{bias} } + \Omega \\
&= \Omega - \left( \frac{1}{M}\overline{var} + \left( 1-\frac{1}{M} \right) \overline{\text{covar} } \right) \\
\text{for~}  & \Omega \defeq \overline{\text{var} } + \Mavg \left( \mathbb{E}_{}\left[ q_{i} \right] - \mathbb{E}_{}\left[ \bar{q} \right] \right)^2
\end{align*}
The first quantity describes the average member error, the second the diversity of the ensemble. The appearance of $\Omega$ in both terms illustrates that there is a trade-off between individual member error and diversity. 
In other words, one cannot maximise diversity without also affecting other parts of the ensemble error. Vice versa, optimising for other components of the error will also affect diversity.

\end{document}