
\documentclass[../main.tex]{subfiles}
\begin{document}

In this chapter, we describe the Random Forest learning algorithm. We first motivate and describe decision trees, which are the basic components of a Random Forest. We then proceed to describe a particular property of decision trees: they are likely to exhibit high variance. While this is undesirable for a learning algorithm, we will see that combining several randomized decision trees into a Random Forest ensemble turns exactly that property into a crucial advantage (see section \ref{sec:unchanged-bias}).
% TODO make sure we get back to this later -- I think the idea was to say that if there's high variance, we also have high room to improve due to diversity?

In its essence, a Random Forest is a collection of randomized decision trees. A decision tree is a data-driven recursive partitioning scheme, combined with a means to produce a prediction based on the training points in a partition cell. The Random Forest prediction then is an aggregate of the predictions of all individual trees.
% TODO also don't uppercase RFs?

\section{Decision Trees}
\label{sec:decision-trees}

As described in section \ref{sec:supervised-learning}, we are interested in learning algorithms that, given some training data, produce a model that is able to predict a reasonable outcome when queried with a previously unseen example. 

% \marginnote{TODO decision tree construction pseudocode}

One intuitive approach is to consider the examples in the training data that are, in some sense, "close" or "similar" to the query point. Then, one might claim that the outcome for the query point must surely be similar to the outcomes of the similar points -- which we already know. Indeed, finding a proper notion of "closeness" is at the heart of many machine learning algorithms such as $k$-Nearest-Neigbours, $k$-Means, etc.
% TODO some examples other than k--

% TODO decision tree always lowercase
Constructing a decision tree means recursively partitioning the input space $\mathcal{X}$, guided by the training data $D$. Then, given a query $X$, we check the partition cell that $X$ belongs to and all the training examples that are in it. These are the examples we consider "close" to $X$. The tree's prediction will be an aggregation of the outcomes of all training points in that cell. 

Because we are recursively partitioning the input space, we have at hand a tree structure of decision rules. The cells of the resulting partition are the leaves of the tree. The non-leaf nodes are also referred to as \textit{decision nodes}, but there is no inherent difference between leaf and non-leaf nodes. We will use the terms \textit{leaf} and \textit{cell} interchangeably, depending which aspect we want to emphasize.


% \marginfigfromsource{fig:decision-tree-structure}{symlinks/illustrations/decision-trees/tree-structure}

\begin{marginfigure}
    \label{fig:decision-tree-partition}
    \includegraphics[width=\textwidth]{figma-illustrations/decision-tree}
    \caption{
        Rendering of a decision tree structure. Each inner node corresponds to a partitioning of the parent edge. In standard decision trees, this is a binary partition. In other words, the examples are \textit{split} at a certain value threshold in a certain feature dimension.    }
\end{marginfigure}

\marginfigfromsource{fig:decision-tree-boundaries}{symlinks/illustrations/decision-trees/decision-tree-boundaries}
    

In summary, there are the following main components to the implementation of a decision tree:
% TODO lowercase "decision tree" always
\begin{itemize}
    \item The \textit{splitting criterion} to apply recursively to subsets of the training data.
    \item The \textit{stopping criterion} that determines whether a node should be split further. This will determine the depth of the decision tree.
    \item The \textit{leaf aggregation function} that produces a prediction for a specific cell. When using the constructed tree for prediction, this will be the leaf node that the query point is assigned in.
\end{itemize}


\subsection{Centroids are good leaf combiners} 

% TODO don't talk about "points"

We will consider the leaf aggregation function first. Consider a parent node $P$ that, due to some split, was partitioned into the disjoint union $L~\dot{\cup}~R$.
Let $y_{P}, y_{L}$ and $y_{R}$ be the output values of the parent and the two new leaf nodes, produced by the leaf aggregation function. Since the leaf output is constant over a single cell, the gain in loss due to a split is the difference between the loss of the parent node and the sum of losses of the two individual child nodes.
For brevity, we write $\ell_{P}(y) \defeq \sum_{i \in P} \ell(y, y_{i})$.
\begin{align*}
\text{\textit{Loss Gain}: } \hspace{1em} &
\sum_{i \in P} \ell(y_{P}, y_{i}) - \left(  \sum_{i \in L} \ell(y_{L}, y_{i}) + \sum_{i \in R} \ell(y_{R}, y_{i}) \right) \\[1.5em]
&= \ell_P(y_P) - \left(\ell_L(y_L) + \ell_R(y_R) \right)
\end{align*}
In order for a split to yield positive loss gain, the leaf aggregation function needs to be such that the loss does not increase as constraints are removed, i.e. the set of considered examples is reduced. Recall that $\bar{z}$ is a centroid with respect to a loss function $\ell$ and a set of outcomes $P$ if and only if $\bar{z} = \arg\min_{z} \sum_{i \in P} \ell(z, y_{i}) = \arg\min_z \ell_P(z)$ (\ref{prelims:centroids}).
\begin{lemma}
\label{lemma:loss-gain-nonnegative}
For a loss function $\ell$, if the leaf aggregator of a node $P$ is $y_{P} \defeq \arg\min_{z} \sum_{i \in P} \ell(z, y_{i})$, i.e. the centroid with respect to $\ell$, the loss gain is nonnegative.
% TODO double-check ~positive~ -> nonnegative.
\end{lemma}
\begin{proof}
Let $\ell_{P}(y) \defeq \sum_{i \in P} \ell(y, y_{i})$. Since $P = L \dot{\cup}R$, we need to show that $\ell_{P}(y_{P}) = \ell_{L}(y_{P}) + \ell_{R}(y_{P}) \geq \ell_{L}(y_{L}) + \ell_{R}(y_{R})$. Assume $\ell_{L}(y_{P}) < \ell_{L}(y_{L})$. This contradicts the definition of $y_{L}$ as the minimizer, and as such $\ell_{L}(y_{P}) \geq \ell_{L}(y_{L})$. Likewise, we can conclude that $\ell_{R}(y_{P}) \geq \ell_{R}(y_{R})$. Combining the two inequalities yields the statement.
\end{proof}

This rigorously motivates the specific choice of leaf aggregation function. The majority vote is a centroid with respect to the \zeroone-loss for classification, while the arithmetic mean is the centroid with respect to the squared error loss for regression.


% define stopping criteria
% defin leaf aggregations

\subsection{Splitting criteria greedily minimise loss functions}

In the best possible case, all training examples in a given cell correspond to the same (classification) or very similar (regression) outcomes. If a query point then falls within that cell, i.e. it has similar features, one can say with high confidence that the query point should have the same (similar) outcome. In the spirit of greedy optimisation, we aim to split cells such that the resulting child cells are more \textit{pure} with respect to their outcomes. 

% TODO move something of these paragraphs to the margin? 
Note that the notion of local purity is linked to the training error: If a leaf cell is perfectly pure, all training examples in that cell correspond to the same outcome. Hence, the leaf aggregation function, which us usually implemented as some kind of mean, will produce exactly that outcome for any query point that belongs to this cell, in particular any training points. Consequently, for a suitable definition of "error", perfectly pure cells have zero training error.

\marginnote{
% TODO rather, as clustering. see post-it.
The splitting function can be understood as a very simple classifier. Standard decision trees, are restricted to axis-aligned splits that divide the data points by a certain threshold value in a certain feature dimension.
% TODO motivation for that; that this is enough, and also more efficient
Extensions have been proposed to construct oblique (non-axis-aligned) splits or to use more sophisticated classifers instead \cite{todo}. 
% TODO also mention spaeh
In this work, we focus on axis-aligned splits but note that results and insights should transfer to other splitting functions. 
% TODO which, how, why.
}

% TODO diagram with tables, how a split makes the outcomes more pure


Consider a split, parameterised by $\Theta$, that partitions a parent node $P$ into the disjoint union $L_{\Theta} ~\dot{\cup}~R_{\Theta}$. Let $n, n_{L}, n_{R}$ be the cardinalities of the parent and the two child nodes. Let $H$ be an impurity measure. We will select the split that yields the lowest impurity.
$$
\arg \min_{\Theta} ~ \frac{n_{L}}{n} H(L_{\Theta}) + \frac{n_{R}}{n} H(R_{\Theta})
$$
The gain in purity is then the difference between impurities before and after the split.
\begin{align}
\text{\textit{Purity Gain}: } \hspace{1em}
H(P) - \left(  \frac{n_{L}}{n} H(L_{\Theta}) + \frac{n_{R}}{n}H(R_{\Theta}) \right)
\label{eq:purity-gain}
\end{align}
Note that this is different from the gain in \textit{loss} achieved due to a split. 


We now proceed to define two commonly used splitting criteria. These are the Gini Index for classification and Variance Reduction (also known as \textsc{Cart}) for regression \cite{tibshirani_ElementsStatisticalLearning_2017}.
Obviously, a reasonable impurity measure should also also lead to a positive loss gain. Often, this is intuitively clear but a comprehensive explanation appears to be hard to find in the literature, particularly for the case of Gini impurity. We clarify this in the following.

\subsubsection{Variance Reduction}
\marginnote{In binary classification, i.e. if outcomes are in $\{0,1\}$, the squared error reduces to the \zeroone-loss. As such, the mean \zeroone-loss, i.e. the error rate, trivially also is a measure of variance.
% OK this is really quite trivial, not sure whether we should keep
}
\marginnote{
    % TODO attribute sklearn 1.10.7.1 (box) https://scikit-learn.org/stable/modules/tree.html#classification-criteria
Another impurity measure is the entropy, defined as
$$
H_{\text{entr}}(P) \defeq - \sum_{i \in P} p_k(x_i) \log(p_k(x_i))
$$
With a similar argument as for the case of variance reduction, one can see that the entropy splitting criterion amounts to minimising the the cross-entropy loss given as
$$
- \frac{1}{n} \sum_{i}^n \sum_k \mathbf{1}\left[y_i=k\right] \log(p_k(x_i))
$$
}

A commonly used impurity measure for regression is the squared-error variance. 
$$
H_{\text{var}}(P) \defeq \frac{1}{n_{P}} \sum_{i \in P} (y_{i} - y_{P})^2 \hspace{2em} \text{for} \hspace{1em} y_{P} \defeq \frac{1}{n_{p}}\sum_{i\in P}y_{i}
$$
To motivate this impurity measure, it remains to be shown that a split guided by this impurity measure actually reduces the value of a specific loss function and which one that is. Luckily, it is easy to see that this holds for the squared error loss. Plugging the definition into the purity gain (eq. (\ref{eq:purity-gain})) yields
$$
H(P) = \frac{1}{n_{P}} \sum_{i \in P} (y_{i} - y_{P})^2
= \frac{1}{n_{P}}  
\underbrace{
 \sum_{i \in L} (y_{i} - y_P)^2 
 }_{\ell_{L}(y_{P})}
 +
\frac{1}{n_{P}}
 \underbrace{
 \sum_{i \in R} (y_{i} - y_{P}) ^2
 }_{\ell_{R}(y_{P})}
$$
and
$$
\frac{n_{L}}{n_{P}} H(L) +
\frac{n_{R}}{n_{P}} H(R) = 
\frac{n_{L}}{n_{P}} \frac{1}{n_{L}} 
\underbrace{
\sum_{i \in L} (y_{i} - y_{L})^2
}_{\ell_{L}(y_{L})}
~ + ~ 
\frac{n_{R}}{n_{P}} \frac{1}{n_{R}}
\underbrace{
 \sum_{i \in R} (y_{i} - y_{R})^2
}_{\ell_{R}(y_{R})}
$$
By lemma \ref{lemma:loss-gain-nonnegative}, we can directly conclude that the loss gain is positive for any split if the arithmetic mean is used as a leaf combiner.

\subsubsection{Gini Index}
\label{sec:gini-index}

% TODO references:
% - leistner2009 3.1 for def of margin vector, relation to empirical node error
% - sklearn user guide 1.10.7.1 for definition of gini split crit
% - CART
% TODO so, how would this behave if we actually used it for classification? since sqerr sort of reduces to 0-1-loss?

One may suggest a measure of purity as the probability of drawing two different outcomes from the examples in the current cell. 
Let $p_{k} = \prob{P}{k|X}$ be the probability of drawing an example of class $k$ from node $P$.
% TODO ($\prob{.}{.}$ is the estimated density in the node $P$ and alos the leaf combiner)
The probability of drawing one example of class $k$ and one of a different class is $p_{k}(1-p_{k})$.
% TODO parameterise, make region explicit?
The probability of drawing two examples of \textit{any} two different classes then is the \textit{Gini index}
$$
G \defeq \sum_{k}p_{k}(1-p_{k})
$$
% TODO proper definition, box?
We will now argue that a reduction in the Gini index in fact pushes values $p_{k}$ to the extremes of the probability simplex. 
We perform a slight shift in perspective and consider the classification \textit{margin} (see \ref{def:classifier-margin}) instead of the estimated probabilites. 
\marginnote{
    \textit{Recap}: The classifier margin for class $k$ of an example $X$ is the difference between the model's confidence that $X$ is of class $k$ and the next-best class:
$$
m_{k}(X) \defeq \prob{}{k|X} - \max_{j\not=k} \prob{}{j|X}
$$
    For a pair $(X,y)$ of example and true outcome, the model's prediction is correct iff $m_y(X) > 0$ 
    The vector $m(x) = [m_{1}(x), \dots, m_{K}(x)]^\top$, where $K$ is the total number of classes, is called a \textit{margin vector} iff its components sum to zero. 
}



We will argue that the Gini index split criterion, which finds a split such that the Gini index is reduced, in fact maximises the classification margins.

% TODO something like these stars to distinguish which I derived myself?
\begin{lemma} $\star$ ~ Let $p$ be a probability distribution and $u$ an arbitrary vector
   % TODO not fully arbitrary,needs to be a valid argument for breg 
    Let $G =\sum_{k} p_{k}(1-p_{k})$ be the Gini index. Then $-G$ is the generator for the Bregman divergence
$$
\sBreg{-G}{p}{u} = \sum_{k}(p_{k}-u_{k})^2
$$
\end{lemma}
\begin{proof}
Let $\phi(q) \defeq (-1) \sum_{k} p_{k}(1-p_{k})$. Then, the first equality follows by definition of a Bregman divergence (see \ref{def:bregman-divergence}) and the second equality by arithmetic.
\begin{align*}
\sBreg{-G}{p}{u} &=  
\underbrace{(-1) \sum_{k} p_{k}(1-p_{k})}_{\phi(p)} 
 ~ ~ - ~ ~  
\underbrace{(-1) \sum_{k} u_{k}(1-u_{k})}_{\phi(u)}  
~ ~ - ~ ~  
\underbrace{\sum_{k} (2u_{k}-1)(p_{k}-u_{k})}_{\langle \nabla \phi(u), p-u \rangle} \\  
 &= \sum_{k} (p_{k}-u_{k})^2
\end{align*}
\end{proof}


Note further that maximising the value of the generator function with respect to one parameter $\phi(p)$ while leaving the other fixed also maximises the divergence $\Breg{p}{u}$.\sidenote{TODO explain, at least with intuition. Convexity of $\Breg{.}{.}$ etc}
The sign of the generator value and the sign of the divergence are related in that  $\sBreg{-\phi}{p}{u} = - \Breg{p}{u}$.
This means that minimising the Gini index during splitting maximises component-wise sum of squared errors between $p$ and $u$.
$$
\min G \rightarrow \min \sBreg{G}{p}{u} \rightarrow \max \sBreg{-G}{p}{u} = \sum_{k}\left( p_{k} - u_k \right)^2
$$
% TODO this is a separable bregman divergence, where else have we seen that?
% TODO this seems to be the "generalised square euclidean distance", see https://franknielsen.github.io/BregmanDivergenceDualIGGenConvexity-25Nov2021.pdf / Scalar and separable Bregman divergences
% TODO cite "on the universality of the logistic loss function"

If $p$ is a probability distribution and $u \defeq [\frac{1}{k}, \dots, \frac{1}{k}]^\top$, then $p-u$ is a margin vector and the optimisation corresponds to maximising the classification margins as measured by the squared error.

A common approach in training classification models
is \textit{margin maximisation} \cite{schapire_BoostingFoundationsAlgorithms_2012}
in which we aim to maximise the margin of the true label $m_{y}(X)$. 
A margin loss function $\ell : \mathbb{R} \to \mathbb{R}$ is a \textit{margin-maximising} loss if $\ell'(m_{y}(X)) \leq 0$ for all values of $m_{y}$ \cite{leistner_SemiSupervisedRandomForests_2009}.
\marginnote{
    An example for a margin-maximising loss function is the \textit{hinge loss} defined as
    $\ell(m_{y}(x)) \defeq  \max \{ 0, 1-p \}$. Its subderivative is 
    $$
    \frac{\partial\ell}{\partial p} = \begin{cases}
    -1 & p \leq 1 \\
    0 & \text{else}
    \end{cases}
    $$ and hence it is a margin-maximising loss.
}

% \begin{marginfigure} \label{fig:hinge-loss}
%     \includegraphics[width=\textwidth]{symlinks/illustrations/mini-fn-plots/hinge-loss.png}
%     \includegraphics[width=\textwidth]{symlinks/illustrations/mini-fn-plots/hinge-loss-grad.png}
%     % TODO put these in same figure after all and mention that hinge loss can be seen as upper bound of 0-1-loss
%     % TODO too hard to read, should have different colours
% \end{marginfigure}
%
A decision tree can also be evaluated based on a margin loss.
The empirical error of a decision tree node $P$ with respect to a margin loss $\ell$ can be written as $L(P) = \frac{1}{|P|} \sum_{i \in P} \ell\left(m_{y}(x_{i})\right)$ where $m_{y}(x_{i})$ is the value of the true margin. Then the following holds \cite{leistner_SemiSupervisedRandomForests_2009}.
\begin{align*}
L(P) &= \frac{1}{|P|}\sum_{i \in P} \sum_{k} \ind{y_{i}=k} ~ ~ \cdot ~ ~  \ell   \left( m_y(x_i) \right)  \\
&= \sum_{k}  
\frac{1}{|P|} \sum_{i \in P} \ind{y_{i}=k}
~ ~ \cdot ~ ~ 
\ell(m_{k}(x_{i})) \\
&= \sum_k p_k(x_i) \ell(m_k(x_i))
\end{align*}
Hence we see that the Gini index splitting criterion greedily optimises classification margins and thus margin-maximising losses such as the Hinge loss.

% TODO: i.e. each tree level is sort of a margin boosting step, cf margin analysis of boosting; ADF etc.

\subsection{Splitting criteria as $2$-means clustering}

% TODO so "loss gain" / "purity gain" somethign there is really k-means objective?

If indeed a centroid is chosen as leaf combiner, as is the case in standard random forests, common splitting functions can be seen to be equivalent to a generalised $k$-means for $k=2$. \cite{banerjee} generalise $k$-means to bregman divergences. The derivation is analogous to the classical case for squared error. 

% The Bregman information is the basic measure to generalise $k$-means clustering \cite{banerjee}. For instance, for clustering with discrete assignments (hard clustering), each cluster (partition cell) is represented by a centroid. The task is to find a set of $k$ centroids such that the loss in Bregman information due to quanitisation is minimised. 

Let $I_{\phi}$ be the Bregman information as defined in definition \ref{def:bregman-information}. Let $X$ be a random variable representing data points. Let $M$ be a random variable taking values in $\mathcal{M}$ representing the set of cluster representatives. Then the objective for generalised $k$-means hard clustering is to minimise the loss in Bregman information due to the quantisation induced by $M$:
$$
 \ell_{\phi}(M) \defeq I_{\phi}(X) - I_{\phi}(M)
$$
One can show \cite{banerjee} that
$$
\ell_{\phi}(M) = \mathbb{E}_{\pi}\left[ I_{\phi}(X_{h}) \right]  \approx \sum_{h=1}^k \sum_{x_{i} \in \mathcal{X_{h}}} \mathcal{v}_{i} \Breg{x_{i}}{\mu_{h}}
$$
where $k$ is the number of clusters, $\mathcal{X}_{h}$ are the cells of the clustering, $\mathcal{v}_{i}$ is the distribution of the $x_{i}$ and $\mu_{h}$ is the right Bregman centroid of $\mathcal{X}_{h}$.

Classical $k$-means is a special case of this for the squared-error divergence. The KL-divergence implies the mutual information as a variance and yields \textit{information-theoretic clustering} \cite{todo}. The Ikura-Saito divergence yields the LBG algorithm \cite{todo}.

Particularly relevant for decision trees is the following insight: The choice of Bregman divergence implies a measure of variance (see \ref{def:bregman-information} for examples). Optimising for this measure of variance implies the splitting criterion \sidenote{The leaf combiner, which corresponds to the right Bregman centroid is independent on the chosen divergence, see \ref{def:bregman-centroids}.}. This gives a theoretical rationale for choosing splitting function and leaf combiner in decision trees.

\subsection{Stopping Criteria \& Tree depth}

...


\section{The Random Forest scheme}

% TODO indeed, high variance of ensemble members is "wanted" in ensembling -- else no room for improvement
% in other words, ensembling only "effective" for high variance learners

\marginfigfromsource{fig:variance-vs-tree-depth}{symlinks/illustrations/bias-variance/variance-vs-tree-depth}

The deeper a decision tree becomes, the closer the decision regions will fit the training data. This approximation is in fact guided \textit{only} by the training data. In the extreme case, if the tree is fully grown, each partition cell will correspond to a single example and the outcome for that cell will be the outcome of that example. The tree essentially degenerates to a $1$-nearest-neighbour scheme with respect to the training dataset. This means that trees constructed with different samples $D$ of training datasets from the original distribution $P(X,Y)$ potentially predict quite different outcomes for testing datapoints. This is captured in the concept of model variance as defined in \ref{sec:bias-variance-effects}. In \ref{fig:variance-vs-tree-depth}, one can indeed observe that trees of greater depth have greater variance. At the same time, their fit to the training data improves, which is captured by lower bias.
% so learner variance and correlation are really tightly the same
% \marginnote{
    % TODO extremly randomized trees -> essentially constant variance?
% }

Mitigating this strong dependence on the training data is one of the main motivations of Random Forests. The basic idea is as follows: If we produce several uncorrelated decision trees and average their predictions, then the predictions should exhibit lower variance
\sidenote{
We use this intuition here to motivate the basic components of Random Forests. We will treat effect of variance reduction through combining multiple models thoroughly in \ref{sec:ens-improv-variance-reduction}.
}.  Exactly how we facilitate uncorrelated trees gives rise to the Random Forest scheme.

The basic idea is to introduce randomness into the decision tree construction. This is achieved by two mechanisms:
\begin{itemize}
\item \textbf{Bootstrapping}: Each tree is constructed not on the entire training dataset but a random subset of it. Usually, this \textit{bootstrap sample} is produced by drawing the same amount of points with replacement.
% TODO some considerations on how bootstrapping is actually a vital component
\item \textbf{Random feature selection}: When determining where to split a node, not all features are considered but only a random subset of a certain size.
% TODO maybe some references....
\end{itemize}

Thus, a random forest additionally requires the following parameters:
\begin{itemize}
\item number of trees...
\end{itemize}

% TODO default params are good choice


% TODO for the following subsections: default params/implementation, review on their effects

\subsection{Bootstrapping \& Bagging}

% randomization

\subsection{Feature \& Split selection}

% randomization

\subsection{Number of trees}

\subsection{Depth of trees}

\subsection{Random Forests converge}

Thm **Random Forests converge** ( breiman Thm 1.2) / #Def **Margin function of a random forest** $\mr(\vec{X}, Y)$ (Def 2.1)
As the number of trees increases, for almost surely all sequences $(\Theta_{1}, \dots)$, the generalisation error $PE^\star$ converges:
$$
PE^\star \to \prob{\vec{X}, Y}{\left(\prob{\Theta}{q(\vec{X}, \Theta)=Y} - \max_{j \neq Y} \prob{\Theta}{q(X; \Theta) = j } \right) < 0
} 
= \prob{\vec{X}, Y}{
\mr(\vec{X}, Y)  < 0
}
$$

\subsection{Random Forests do not overfit}

\subsection{Random Forests are consistent}


\subsection{Practical advantages}
parallelisable, oob estimation, feature importance, proximity, ...

\end{document}