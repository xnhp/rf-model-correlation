\documentclass[../main.tex]{subfiles}
\begin{document}


 % TODO move this to chapter on Bregman divergences? since we're not doing any experiments, and this is somewhat specific
% TODO highlight that we're not really for *negative* correlation learning but uncorrelated
% maybe use sth else as section title and then in text "also known as negative correlation learning"

Consider the bias-variance-covariance decomposition for the squared error loss given in \cref{thm:bias-variance-covariance}.
% \begin{align*}
% \mathbb{E}_{D}\left[ (y - \bar{q}(x))^2 \right]  &= (\mathbb{E}_{D}\left[ \bar{q}(x) \right] - y)^2 \\
% & + \mathbb{E}_{D}\left[ \frac{1}{M^2}\sum_{i=1}^M (q_{i}(x) - \mathbb{E}_{D}\left[ q_{i}(x) \right] )^2 \right]  \\
% & + \mathbb{E}_{D}\left[ \frac{1}{M^2}\sum_{i \not=j}^M (q_{i}(x) - \mathbb{E}_{D}\left[ q_{i}(x) \right] )(q_{j}(x) - \mathbb{E}_{D}\left[ q_{j}(x) \right] ) \right] 
% \end{align*}
% TODO remove explicit dependence on x
The covariance term contributes positively to the ensemble error if the outputs of the members are positively  correlated. This suggests the idea that it might be beneficial to train ensemble members in coordination such that their outputs are uncorrelated.
\citeauthor{liu_EnsembleLearningNegative_1999} \cite{liu_EnsembleLearningNegative_1999} provide an implementation of this idea for neural network ensembles called \textit{Negative Correlation Learning} (NCL).
\marginnote{
  Note that the name is misleading here. The goal is not that members are negatively correlated, but \textit{un}correlated.
}
Neural network training involves a \textit{forward} and a \textit{backward} pass. In the forward pass, the neural network in its current state is queried with training data. Its prediction is evaluated according to a loss function. 
Then, the model parameters are updated according to the gradient of the loss function with respect to the parameters. 
%
A common practise is to add a \textit{regularisation term} to this loss function in order to bias the model towards e.g. sparse predictions \cite{tibshirani_ElementsStatisticalLearning_2017}. 
%
In Negative Correlation Learning, forward and backward pass are performed synchronously for all members. This enables using loss functions that depend not only on an individual member but the entire ensemble.

The $i$-th member network is trained using a loss function $e_{i}$ that contains a regularisation term $p_{i}$, which is intended to influence the training of the $i$-th member such that its predictions are uncorrelated with the other members. The hyperparameter $\lambda \in \mathbb{R}$ determines how much weight is put on either of the two components. The training objective for the $i$-th network is defined as
$$
 e_i \defeq ( y - q_{i})^2 ~ ~ + ~ ~ \lambda ~ p_{i}
$$

\marginnote{
    $\bar{q}$ is the arithmetic mean:
    $$\bar{q} = \Mavg q_{i}$$
  The sum of deviations around the mean is zero:
  $$
  \sum_{i=1}^M (\bar{q} - q_{i}) = 0  
  $$
  Omitting one member from the sum implies
  $$
  \sum_{i \not= j} (\bar{q} - q_{i}) = - (\bar{q} - q_{j})
  $$
}
where the first term is the individual error of the $i$-th member and $p_i$ is the regularisation term. \cite{liu_EnsembleLearningNegative_1999} defined $p_i$ as
$$
p_{i} \defeq \left( 
(\bar{q} - q_i)
\sum_{i \not=j}^M (\bar{q} - q_i))
\right)
= - \left(\bar{q} - q_i\right)^2
$$
This is reminiscent in shape of the contribution of the $i$-th member to the covariance term in the bias-variance-covariance decomposition --- however, the expectation over $D$ is replaced with the ensemble combiner $\bar{q}$! Seeing that here the ensemble combiner is the arithmetic mean of member outputs, this means that instead of a variance under variations in $D$, this considers the variance between member model outputs. 

Note that the penalty term corresponds to the contribution of the $i$-th member to the ambiguity (\cf \ref{thm:ambig-effect-decomp}).
% TODO these formulas dont make too much sense right now -- go through notes again with care. I think brown and liu_EnsembleLearningNegative_1999 are not exactly equivalent.
% TODO highlight somewhere that this links bias-variance-covariance decomp and ambiguity -- i.e. expression via covarance is actually artifact of squared error -- and more importantly, that we are comparing model outputs and not losses -- which, at first, seems confusing
Indeed, as \citeauthor{brown_ManagingDiversityRegression_2005} \cite{brown_ManagingDiversityRegression_2005} noted, the NCL approach can be motivated more soundly via the ambiguity decomposition. 

% The error of the full ensemble can be written as \sidenote{We additionally use a factor of $\nicefrac{1}{2}$. This makes it easier to express gradients of this function and is equivalent for optimisation. }
% $$
% e_{\text{ens}} \defeq \nicefrac{1}{2} (\bar{q}-y)^2 = \Mavg \nicefrac{1}{2}(y - q_{i})^2 ~ ~ - ~ ~ \Mavg \nicefrac{1}{2}(\bar{q} - q_{i})^2
% $$
\begin{definition} (NCL neural network objective for squared error \cite{brown_ManagingDiversityRegression_2005}) The loss function of the $i$-th neural network ensemble member is defined as
$$
e_{i}(y,x) \defeq (y - q_i)^2 ~ ~ - \lambda (\bar{q} - q_i)^2
 % TOOD why sum over members here when we define ell_i? should be removed?
$$
\end{definition}
The penalty coeffient $\lambda$ smoothly interpolates between training $q_{i}$ to either maximise its individual performance or the ensemble performance. For sake of simplicitly, let us assume a factor of $\nicefrac{1}{2}$ on the objective. This yields an equivalent optimisation problem, but allows us to to express the gradients in a simple form.
$$
\frac{\partial e_{i}}{\partial q_{i}} = \frac{1}{M}
\left(
(q_{i}-y) - \lambda (q_{i} - \bar{q})
\right)
$$
and
$$
\begin{align}
\lambda = 0 ~~&\rightarrow~~ \frac{\partial e_{i}}{\partial q_{i}} = \frac{1}{M}(q_{i}-y) = \frac{1}{M} \frac{\partial e_{i}}{\partial q_{i}} \\
\lambda = 1 ~~&\rightarrow~~ \frac{\partial e_{i}}{\partial q_{i}} = \frac{1}{M}(\bar{q} - y) = \frac{\partial e_{\text{ens}}}{\partial q_{i}}
\end{align}
$$

Several authors have attempted to generalise Negative Correlation Learning or transfer it to other loss functions. For instance, \citeauthor{webb_EnsembleNotEnsemble_2019} \cite{webb_EnsembleNotEnsemble_2019} directly prove an ambiguity decomposition for the KL-divergence $K$ and propose an analogous objective
% $$
% e_{i}(x,y) \defeq \Mavg K(y \mid\mid q_{i}) ~ ~ - ~ ~ \lambda ~ \Mavg K(\bar{q} \mid\mid q_{i})
% $$
% TODO consistent notation for KL-divergence
where $\bar{q}$ is the geometric mean combiner. The proof is not trivial and does not give insight into whether such a decomposition and learning strategy might also exist for other loss functions.

\citeauthor{buschjager_GeneralizedNegativeCorrelation_2020} \cite{buschjager_GeneralizedNegativeCorrelation_2020} approached the problem by attempting to derive a generalised decomposition of the ensemble error that incorporates diversity. The basic idea is to consider a Taylor approximation around the ensemble combiner, which is assumed to be the arithmetic mean combiner. $\bar{q} \defeq \mathbb{E}_{\Theta}\left[ q_{\Theta} \right] \approx \frac{1}{M}\sum_{i=1}^M q_i$.
\begin{align*}
\mathbb{E}_{}\left[ \ell(y, q) \right]  &= 
\mathbb{E}_{}\left[ \ell(\bar{q}) \right] 
+ \mathbb{E}_{}\left[ (q - \bar{q})^\top \nabla_{q^\star}(\ell (\bar{q})) \right]  \\
&+ \mathbb{E}_{\Theta}\left[ \nicefrac{1}{2} (q-\bar{q})^\top\nabla^2_{\bar{q}}(\ell(q^\star))(q-\bar{q}) \right] \\
&+ \mathbb{E}_{}\left[ R_{3} \right] 
\end{align*}
$R_{3}$ is the remainder of the Taylor approximation, which vanishes if the third derivative of $\ell$ is zero.
By definition of $\bar{q}$, $\mathbb{E}_{\Theta}\left[ \nabla_{\bar{q}}(\ell(\bar{q})) \right] = \nabla_{\bar{q}}(\ell(\bar{q}))$ and $\mathbb{E}_{\Theta}\left[ q-\bar{q} \right] = 0$ and thus the second term vanishes.

The expectations are approximated as follows:
$$
\bar{q} = \mathbb{E}_{{\Theta}}\left[ q_{\Theta} \right] \approx \Mavg q_{i}
$$
\begin{align*}
\mathbb{E}_{\Theta}\left[ \nicefrac{1}{2} (q-\bar{q})^\top\nabla^2_{\bar{q}}(\ell(\bar{q}))(q-\bar{q}) \right] \approx \nicefrac{1}{2}\Mavg d_{i}^\top D d_{i} \\
\text{~~for $D \defeq \nabla^2_{\bar{q}}(\ell(\bar{q}),y)$ and $d_{i} \defeq (\bar{q} - q_i)$}
\end{align*}
\vspace{0.5em}
$$
\mathbb{E}_{\Theta}\left[ R_{3}(x) \right]  \approx \tilde{R}
$$


Note that here they \textit{assume} the expected model $\bar{q}$ to also be the ensemble combiner, i.e. $\bar{q} \approx \Mavg q_{i}$. 
% TODD can we relate this to the qstar lemma?
Based on this, they propose the following generalised training objective.
\begin{definition} (Generalised NCL objective as proposed by \cite{buschjager_GeneralizedNegativeCorrelation_2020})
    \label{def:buschj-ncl-objective}
$$
e_i \defeq \Mavg \ell(y,q_{i}) - \nicefrac{1}{2} \Mavg d_{i}^\top D d_{i}
$$
where $D \defeq \nabla^2_{\bar{q}}(\ell(y, \bar{q})$ and $d_{i} \defeq (q_{i} - \bar{q})$
\end{definition}

This is based on the assumption that the remainder to the Taylor approximation $R_{3}$ is negligibly small. Further, it does not apply to all loss functions. Let us consider some examples of commonly used loss functions.
\marginnote{
Squared error loss:
$$
\ell(y,x) \defeq (y-x)^2
$$
Negative log-likelihood:
$$\ell(z,y) \defeq - \sum_{i}^k y_{i}\log(z_{i})$$
Cross-entropy loss:
$$\ell(z,y) \defeq - \sum_{i}^k y_{i} \log \frac{e^{z_{i}}}{\sum_{j}e^{z_{j}}}$$
}
\begin{itemize}
\item For the squared error, the third derivative vanishes and thus the decomposition is exact.
\item For the negative log-likelihood,
the third derivate is not bounded and thus this decomposition can not be used.
\item For the cross-entropy loss, 
the third derivative is bounded and while the decomposition is not exact, it can be used for approximation. % TODO clarify
\item For any other loss function, this would have to be checked.
% TODO clarify
\end{itemize}

It is evident however, {that we already have a fully general ambiguity decomposition at hand}, namely the ambiguity-effect decomposition (\cf \ref{thm:ambig-effect-decomp}). 
This decomposition is exact and holds for \textit{any} loss function (including the \zeroone-loss). 
For Bregman divergences, this reduces to the ambiguity decomposition.
% TODO direct comparison to those above, e.g. NLL, CE -- need to study wood23 a bit more
%
We claim that the adequate generalisation of the NCL objective follows this structure.
\begin{proposition} $\star$ We propose the following generalisation of the NCL neural network objective. For general loss functions $\ell$:
$$
e_{i} \defeq \ell(y, q_{i}) ~ ~ - \lambda \left( 
 \ell y, q_{i}) - \ell(y, \bar{q})
\right)
$$
For Bregman divergences:
$$
e_{i} \defeq \Breg{y}{q_{i}} ~ ~ - \lambda  \Breg{\bar{q}}{q_{i}}
$$
\end{proposition}
NCL for the squared error loss as originally proposed \cite{liu_EnsembleLearningNegative_1999,brown_ManagingDiversityRegression_2005}, as well as for the KL-divergence \cite{webb_EnsembleNotEnsemble_2019} are special cases of this.

This provides a general framework for Negative Correlation Learning with arbitrary loss functions. Because it is founded on the exact bias-variance-diversity decomposition, this also yields a natural and intuitive means for understanding and analysing Negative Correlation Learning and its effects.

% TODO maybe move this to the outlook section
Note that the weighting strategies of \cref{sec:guided-parallel-training} implicitly optimise a very similar objective. The construction procedure of individual decision trees greedily optimises its own performance (\cf \ref{sec:decision-trees}), which corresponds to the first term in the NCL neural network objective. The introduction of example weights based on the ensemble diversity influences the decision tree construction to improve the ambiguity, which corresponds to the regularisation term. 

Although the term \textit{Negative Correlation Learning} in the literature refers specifically to neural networks, we can now see that it is rather a style of training ensemble members with respect to the ambiguity decomposition.
To the best of our knowledge, only one other algorithm has been published that realises this: \citeauthor{buschjager_ThereNoDoubleDescent_2021} \cite{buschjager_ThereNoDoubleDescent_2021} proposes to refine the leaf predictions in a given Random Forest using using gradient descent according to the objective defined in definition \ref{def:buschj-ncl-objective}.
While an experimental evaluation has to be left for future work, we note that the case of KL-divergence was already investigated empirically in detail \cite{webb_EnsembleNotEnsemble_2019}.

% *only* if we really have the time to spare
% \subsection{Experiments}

% As a proof of concept, we perform Negative Correlation Learning with small neural networks based on the cross-entropy loss (ref{thm:cross-entropy-decomp}).

% \marginnote{
%    Experiment setup, NN architecture? (also see margintable command/environment)
% }

% TODO some remarks about peculiarities with NNs..?
% TODO looking at margin plots / hists would also be interesting here

% TODO NCL with zero-one-loss? One for which decomp does not hold. would be cool. 

\end{document}