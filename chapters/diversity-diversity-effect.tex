\documentclass[../main.tex]{subfiles}
\begin{document}



% TODO write somewhere that bayes error can also be considered a variance (see Adlam22 eq (5)
\section{Generalising Ambiguity} 

% TODO cite 
%- krog_NeuralNetworkEnsembles
% - wood23

% TODO clarify that were omitting E_(X,Y) herere
% TODO in other places, randomness of models is within RV D, here we use Theta

In section \ref{sec:bias-variance-effects}, we have derived a generalised bias-variance decomposition by considering the effect on the loss of using a random variable instead of its non-random centroid. 
For the variance, this would be the expected difference in loss between using a model dependent on the training input $D$ and the expected model, where the expectation is over $D$. 
Consider now an ensemble of learners, in which each learner is constructed according to a random parameter $\Theta$. Similar to bias and variance, we may consider the distribution of models over $\Theta$ around a central model $\bar{q}$ that is non-random with respect to $\Theta$. 
The expected \textit{effect} of using the central model instead of some single member is expressed as follows.
$$
\mathbb{E}_{\Theta}\left[   \ell(Y, q_{\Theta}) -  \ell(Y, \bar{q})) \right] \approx \Mavg \ell(Y, q_i) - \ell(Y, \bar{q})
$$
Using the same strategy as for the bias-variance-effect decomposition of theorem \ref{thm:bias-variance-effect-decomp}, we can use this to formulate a decomposition of the generalisation error with respect to the central model.

\marginfigfromsource{fig:ambiguity-spread}{symlinks/illustrations/ambiguity/rf-ambiguity}

% TODO need to somewhere make clear that we write without noise everywhere

\begin{theorem} \label{thm:ambig-effect-decomp} (Ambiguity-Effect decomposition \cite{todo}) For any loss function $L$, target label $Y$, ensemble members $q_{1}, \dots, q_{M}$ with combiner $\bar{q}$
\begin{align*}
\ell(Y, \bar{q}) &= \Mavg \ell(Y, q_{i}) -
\underbrace{\left(\Mavg \ell(Y, q_{i}) - \ell(Y, \bar{q})\right)}_{\text{Ambiguity-Effect / Ensemble Improvement}} \\
&= \Mavg \ell(Y, q_i) - \Mavg \LE{\bar{q}}{q_i}
\end{align*}
% TODO use LE notation
\end{theorem}
If the ensemble combiner $\bar{q}$ is chosen to be the central model, this is a decomposition of the ensemble generalisation error and a generalisation of the ambiguity decomposition described in section \ref{sec:ambiguity}. 
% TODO show that majority vote is actually centroid to 0-1-loss (have that in touchnotes somewhere)
We can then also find an intuitive interpretation of ambiguity-effect: It is the effect of ensembling on the error. If we consider the members to be constructed according to a parameter $\Theta$, a reasonable measure of the member performance is its loss in expectation over the parameter distribution: $\mathbb{E}_{\Theta}\left[ L(Y, q(X; \Theta))  \right] \approx \Mavg L(Y, q(X;\Theta_{i})$. What we gain or lose from using an ensemble $\bar{q}$ over just a single member model is exactly measured by ambiguity-effect. Due to this, this quantity is also known as \textit{ensemble improvement} \cite{theisen, and others?}.

% TODO some sentence on how this was defined as ambiguity-effect in krogh or sth


% TODO wood23 motivate this from perspective that ambig-eff is special case of bias-variance decomp

% \begin{marginfigure} \label{fig:ambiguity-spread}
%     \includegraphics[width=\textwidth]{symlinks/illustrations/ambiguity/rf-ambiguity.png}
%     \input{symlinks/illustrations/ambiguity/rf-ambiguity.tex}
% \end{marginfigure}



%\begin{theorem} \label{thm:ambiguity-decomp} (Ambiguity decomposition \cite{todo:krogh}) For any of the loss functions of \ref{thm:the-trick}, the ambiguity-effect term reduces to the target-independent \textit{ambiguity}
%\begin{align*}
%L(Y, \bar{q}) &= \Mavg L(Y, q_{i}) - \left(\Mavg L(Y, q_{i}) - L(Y, \bar{q})\right) \\
% &= \Mavg L(Y, q_{i}) - \Mavg L(q_{i}, \bar{q})
%\end{align*}
%\end{theorem}
% TODO has been discussed in the literature earlier

Similar to variance, ambiguity and ambiguity-effect are measures of spread. 
Variance measures the spread of training error across models trained with different draws of the training dataset $D$ around a model that is a centroid with respect to the distribution of $D$. Similarly, ambiguity measures the spread of individual member model errors around a model that is centroid with respect to the distribution of $\Theta$, namely the combiner $\bar{q}$. In case of squared loss, this is indeed the statistical variance around the arithmetic mean. For other losses, this is a different quantity.
% can maybe move the above into a side note.



%TODO need to argue more that this is indeed centroid? should come from generalised / bregman etc.?
\section{The Diversity-Effect decomposition} \label{sec:bias-variance-diversity-decomp}

The ambiguity decomposition divides the ensemble error into the average member error and the variance among members. How does this relate to the well-known bias-variance decomposition? Indeed, nothing prevents us from applying the bias-variance decomposition of theorem \ref{thm:bias-variance-effect} to the error of an individual member, resulting in a decomposition into \textit{average} bias, \textit{average} variance and expected ambiguity, which is also referred to as \textit{diversity} \cite{wood23}.
$$
\begin{align}
\mathbb{E}_{}\left[ \ell(y, \bar{q} ) \right]  &=  
\mathbb{E}_{}\left[ \Mavg \ell(y, q_{i} ) \right]  
+  
\mathbb{E}_{}\left[ \Mavg \LE{\bar{q}}{q_{i}} \right]  \\
&= \mathbb{E}_{}\left[ \Mavg \LE{q_{i}^\star}{y^\star } \right]  + \mathbb{E}_{}\left[ \Mavg \LE{q_{i}^\star}{q_{i}} \right] + \mathbb{E}_{}\left[ \Mavg \LE{\bar{q}}{q_{i}} \right] 
\end{align}
$$
In summary, the decomposition is given in the following theorem. Note that it holds for \textit{any} loss function. 

\begin{marginfigure}
    \includegraphics[width=\textwidth]{symlinks/zero-one-plots/bvd-decomps/spambase-openml/bvd-individual/standard-rf-classifier}
    \label{fig:spambase-standard-rf-classifier-bvd}
    \caption{
        foo bar baz, avg bias, variance constant, diversity increases
    }
    % TODO legend?
    % TODO here, it would actually be good to increase x axis range to show that the entire thing stabilises
\end{marginfigure}

\begin{theorem} (Bias-Variance-Diversity-Effect decomposition)
    % TODO repeat what star, bar, Y is
    % TODO notation
\begin{align*}
    % TODO write E_(X,Y),D instead
    \mathbb{E}_{(X,Y),D}\left[ \ell(Y, \bar{q}) \right] &= 
    \underbrace{ \mathbb{E}_{Y}\left[ \ell(y, y^\star) \right] }_{\text{noise}} \\
    &+
    % \mathbb{E}_{\Theta, Y}\left[ L(Y, q_{\Theta}^\star) - L(Y, y^\star)\right] % avg bias-effect
    \underbrace{
        \frac{1}{M} \sum_{i=1}^M \mathbb{E}_Y\left[\ell\left(Y, q_i^*\right)-\ell\left(Y, Y^*\right)\right]
    }_{\overline{\text{bias-effect}}} \\
    &+
    \underbrace{
        \frac{1}{M} \sum_{i=1}^M \mathbb{E}_D\left[\mathbb{E}_Y\left[\ell\left(Y, q_i\right)-\ell\left(Y, q_i^*\right)\right]\right]
    }_{\overline{\text{variance-effect}}} \\
    % \mathbb{E}_{\Theta, D, Y}\left[ L(Y, q_{\Theta}) - L(Y, q_{\Theta}^\star) \right]  % avg variance-effect
    &- % minus!!
    % \mathbb{E}_{D, Y, \Theta}\left[ L(Y, q_{\Theta} - L(Y, \bar{q})) \right] % avg div-effect
    \underbrace{
    \mathbb{E}_D\left[\mathbb{E}_Y\left[\frac{1}{M} \sum_{i=1}^M\left[\ell\left(Y, q_i\right)-\ell(Y, \bar{q})\right]\right]\right]
    }_{\text{diversity-effect}}
\end{align*}
% TODO use LE notation
\end{theorem}

\paragraph{Unchanged bias and reduction in variance}
\label{sec:unchanged-bias}

In section \ref{sec:ensemble-learning-motivation} we gave arguments for how in specific cases, the ensemble bias equals the bias of any member. We now have the tools to show this in a more general manner \cite{wood23}.

For the ensemble bias, application of the ambiguity-effect decomposition (see \ref{todo}) to a set of centroid models $q_{i}^\star$ yields:
% TODO also use LE (loss-effect) notation in other places -- but make sure we're not confusing anything
$$
\underbrace{
\LE{y}{\bar{q}^\star} 
}_{\text{ens. bias}}
= 
\underbrace{
\Mavg \LE{y}{q_{i}^\star}
}_{\text{avg. bias}}
- 
\underbrace{
\Mavg\LE{\bar{q}^\star}{q_{i}^\star}
}_{\Delta}
$$
For the ensemble variance, application of the diversity-effect decomposition (see \ref{todo}) while substituting $y \gets \bar{q}^\star$:
$$
\underbrace{
\mathbb{E}_{D}\left[ \LE{\bar{q}^\star}{\bar{q}} \right]
}_{\text{ens. var.}}
 = 
\underbrace{
\Mavg \LE{\bar{q}^\star}{q_{i}^\star} 
}_{\Delta}
+ 
\underbrace{
\Mavg \mathbb{E}_{D}\left[ \LE{q_{i}^\star}{q_{i}} \right] 
}_{\text{avg. var.}}
- 
\underbrace{
\mathbb{E}_{D}\left[ \Mavg \LE{\bar{q}}{q_{i}} \right] 
}_{\text{diversity}}
$$
Due to Lemma (\ref{qstars-same}) which states that in homogeneous ensembles $q_{i}^\star = q_{j}^\star = \bar{q}^\star$, we can conclude that $\Delta = 0$. 
\begin{corollary} For homogeneous ensembles, we can conclude the following:
\begin{itemize}
    \item The ensemble bias is equal to the average member bias:
$$
\Delta = 0 ~ ~ \rightarrow ~ ~ \LE{y}{\bar{q}^\star} = \Mavg \LE{y}{q_{i}^\star}
$$
\item \textit{Diversity is a component of ensemble variance}. The other component is the average member variance. In other words, \textit{ensemble variance reduction is measured exactly by diversity}. 
\item Diversity is bounded from above by the average member variance.
\end{itemize}
\end{corollary}



% TODO what about note in wood23 that \mathbb{V} and L are not necessarily the same?

% TODO diversity is expected value of ambiguity

% TOOD trying to derive a decomp for 01-loss, KohaviWolpert complain that variance should be nonnegative -- well, we have that here



\section{Diversity for Bregman Divergences} \label{sec:bregman-divergences}

% TODO super interesting: bregman information, is just sample variance for squared loss, mutual information for KL-divergence, also interpretable for Itakura-Saito -- so here it sort of also appears in the purity measures
% https://jmlr.csail.mit.edu/papers/volume6/banerjee05b/banerjee05b.pdf
% also note their application of Jense's inequality: the difference between the two sides is exactly equal to the bregman information
% -- could this be the ensemble improvement?!??!
% ah yes, okay, we can already sort of read that between the lines from what we already have

% TODO somewhere: table with common loss funcitons, their generators, combiners

% TODO review on breggies: applications e.g. in generative modelling, GANs? 
% chatbot says:
% Good question. One field where maximizing the generator function can be important is in generative modeling/unsupervised learning tasks.
% Specifically, in generative adversarial networks (GANs):
% - GANs aim to learn a generative model G that produces samples resembling the real data distribution Pdata.
% - G competes against a discriminative model D whose goal is to distinguish real from generated samples. 
% - D's training objective is to maximize the probability of assigning the correct label (real vs fake) to both real and generated samples.
% - In other words, D aims to maximize the divergence metric (usually JS divergence) between Pdata and Pg (the generated distribution).
% - Maximizing this divergence encourages D to become better at discrimination, which provides stronger feedback to help G improve.
% So in GAN training, the discriminator D plays the role of maximizing the generator function (divergence metric), in order to facilitate learning the best generative model G. Maximizing the divergence is important for the adversarial process that improves both models.
% Other areas like domain adaptation may also involve maximizing divergences between distributions to learn robust features/mappings across domains.

Note that bias-effect, variance-effect and ambiguity-effect are all of the form $\mathbb{E}_{}\left[ \LE{\circ}{\square} \right]$ and depend directly on the target label $Y$. 
With variance-effect, we have captured the \textit{effect} of variations between different training datasets) on the prediction error.
With ambiguity-effect, we have captured the effect of variations between the different member models on the prediction error.
We have already seen that for the squared error, the variance-effect coincides with familiar notion of "statistical" variance between the predictions.

% TODO put somewhere else 
% This is convenient, since we can then estimate variance (and its effect) independently of the target label $Y$.
% This means that variance can be compared across different datasets.
% % TODO re-check that this is true
% The same applies to ambiguity \sidenote{Bias will always be indirectly dependent on the target labels via the expected label.}.
% Also, labelled data can be scarce in practise, while examples alone may be easier to obtain. 

We will now define a class of losses for which the loss-effects reduce to the loss between the two objects. This class covers many widely used loss functions and thus allows us to formulate a unified bias-variance-decomposition. 

% TODO put somewhere else
% However, for some other losses -- in particular the 0-1-loss for classification -- such a decomposition is proven to not exist (see \cite{todo}). In these cases, we can still consider the more general effect decompositions (see \ref{sec:todo}).



Bregman divergences have the one key property that their loss-effect terms collapse. That is, with bregman divergences, we have, for the the proper choice of $Z, Z'$:
$$
\mathbb{E}\left[\LE{Z'}{Z}\right] = \Breg{Z'}{Z}
% TODO check order of arguments
$$
This is given by the following two lemmas. 
% TODO highlight how we are losing one expectation, and which one that is
% TODO motivate more on why centroids

% #TODO its not really harder, just need to understand Thm 0.1 of Pfau13 (and maybe mention that this is essentially a statement about effect terms, but wasnt explicitly considered)
% if we want to make the point about centroids etc clearer, we already give the basis for thm0.1, so might as well do that.
% #todo also provide thm0.1 (needed for bias it seems, but first check that they really are not the same)
% the following is required for variance and ambiguity
\begin{lemma} \label{thm:bregman-collapse-bias} (\cite{pfau}, Theorem 0.1 (b))
Let the generator $\phi: \mathcal{S} \to \mathbb{R}$ be a strictly convex, differentiable function. Let $Y$ be a random variable on $\mathcal{S}$. Then, for any $q \in \mathcal{S}$, it holds that
% #todo will need to define bregman centroid. what kind of centroid do i need for variance lemma?
$$
\Breg{y^\star}{q} = \mathbb{E}_{}\left[ \Breg{Y}{q}  - \Breg{Y}{y^\star} \right]
$$
if $y^\star$ is the right Bregman centroid.
\end{lemma}

This shows that bias-effect collapses to bias for Bregman divergences: 
$$\Breg{y^\star}{q^\star} = \mathbb{E}_{}\left[ \Breg{Y}{q^\star} - \Breg{Y}{y^\star} \right]$$

\begin{lemma} \label{thm:bregman-collapse-variances} ($\star$, Generalised from \cite{ref:wood23})
    % TODO check whether this is worthy of a star  -- also shown in pfau I think?
Let $\vec y$ be a random vector. Let $\vec q_{Z}$ be a random vector dependent on another random variable $Z$. Then it holds that
% TODO not sure whether this is correct, seems to have the exact same shape as above...
$$
\Breg{q^\star}{q} = 
\mathbb{E}_{Y}\left[ \Breg{Y}{q} - \Breg{Y}{q^\star} \right]  \\[1em]
$$
if  $q^\star$ is the left Bregman centroid.
\end{lemma}

\begin{proof}
\begin{align*}
\mathbb{E}_{Y,Z}\left[ \Breg{y}{q} - \Breg{y}{q^\star} \right]  &= \dots
\end{align*}
\end{proof}
% TODO give proof here --relies only on linearity and "dual expectation"
% TODO note that this is dual expectation, and repeat lemmas from paper indicating that the dual expectations are indeed centroids
% TODO more on how this is all about variation around centroids and what the centroids are in example of variance and ambiguity
% TODO bias-variance-diversity decomp
% TODO highlight somewhere that this is a slightly different way of showing the decomp -- was previously shown by directly showing bias-variance decomp for it; here we start with effect decomp which trivially holds and then apply bregman collapse trick (which is inherent in the other proof too, but certainly not as clear)

% TODO write D, Theta in both equations, needs to be clearer I guess
This shows that variance- and ambiguity-effect collapse to variance and ambiguity. The variance (in the bias-variance sense) is the variance of the estimates $q_D$ dependent on $D$ with respect to different realisations of $D$ around its $D$-centroid. 
$$
\text{For $q^\star = \mathcal{E}_D\left[q_D\right]$:}\hspace{1em}
\Breg{q^\star}{q} = \mathbb{E}_{}\left[ \Breg{Y}{q^\star} - \Breg{Y}{q} \right]
$$
Ambiguity/Diversity is the variance of the estimates $q_\Theta$ dependent on $\Theta$ with respect to different realisations of $\Theta$ around its centroid.
$$
\text{For $\bar{q} = \mathcal{E}_\Theta\left[q_\Theta\right]$:}\hspace{1em}
\Breg{\bar{q}}{q} = \mathbb{E}_{}\left[ \Breg{Y}{\bar{q}} - \Breg{Y}{q} \right]
$$
% TOD do these have a geometric intuition?

% TODO also give bregman ambiguity decomp somewhere
% then mention that krogh showed this for KL-divergence and Geurts (as mentioned in Louppe p67) for squared error.

This yields a generalised bias-variance-diversity decomposition for bregman divergences as a special case of the corresponding effect decomposition \ref{todo}.

% TODO notation, (re)define symbols
% TODO clarify: expectation over X
% TODO clarify: q depends on X (and D?), q* depends only on X
% #todo writing E_Y|X here, reconcile with other things
% \begin{theorem} (\textit{Bias-variance-decomposition for Bregman divergences}) \label{thm:bregman-bias-variance-decomp}
% $$
% \mathbb{E}_{(X,Y), D}\left[ \Breg{Y}{q} \right] 
% = \underbrace{\mathbb{E}_{Y|X}\left[ \Breg{Y}{y^\star} \right]   }_{\text{noise}}
% + \underbrace{\Breg{y^\star}{q^\star} }_{\text{bias}}
% + \underbrace{\mathbb{E}_{D}\left[ \Breg{q^\star}{q} \right] }_{\text{variance}}
% $$
% % #todo make sure the order arguments is as requried by the lemmas
% \end{theorem}

\begin{theorem} (Bias-Variance-Diversity decomposition for Bregman divergences)
    \label{thm:bregman-diversity-decomp}
    % TODO define qstar and qbar in terms of (dual) expectations (left/right centroids)
    % TODO define Ybar -- should this be Ystar?
\begin{align*}
\mathbb{E}_{(X,Y), D}\left[ \Breg{Y}{q} \right]  
&= \underbrace{\mathbb{E}_{Y|X}\left[ \Breg{Y}{y^\star} \right]   }_{\text{noise}} \\
&+
\underbrace{
    \frac{1}{M} \sum_{i=1}^M B_\phi\left(\overline{\mathbf{Y}}, \mathbf{q}_i^*\right)
}_{\overline{\text{bias}}} \\
&+
\underbrace{
    \frac{1}{M} \sum_{i=1}^M \mathbb{E}_D\left[B_\phi\left(\mathbf{q}_i^*, \mathbf{q}_i\right)\right]
}_{\overline{\text{variance}}} \\
&-
\underbrace{
    \mathbb{E}_D\left[\frac{1}{M} \sum_{i=1}^M B_\phi\left(\overline{\mathbf{q}}, \mathbf{q}_i\right)\right]
}_{\text{diversity}}
\end{align*}
\end{theorem}

% TODO describe terms
% TODO give a plot here how avg-bias, avg-variance do not change but diversity improves

% TODO note on how we require special ensemble combiner according to dual expectation for this to hold
% give table for combiners for squared-error KL-divergence etc

\paragraph{The ensemble combiner for Bregman divergences} ...
% !! TODO primal averaging can *hurt* performance, cf. adlam22

% TODO variance reduction cf adlam22

\paragraph{Ensemble improvement for Bregman Divergences}

In section \ref{sec:ensemble-learning-motivation}, we have used Jensen's inequality to show that the ensemble improvement is non-negative for some cases.
$$
\mathbb{E}_{{\Theta}}\left[ \ell (q_{\Theta}(X),Y) \right]  -
\ell(\mathbb{E}_{\Theta}\left[ q_{\Theta}(X) \right] ,Y ) \geq 0
$$
It is evident, that the Jensen gap is but a special case of ambiguity-effect (\ref{thm:ambiguity-effect-decomp}) for $\bar{q} \defeq \Mavg q_{i}$ and convex loss functions. 
This shows that the ensemble loss is always smaller-equal than the expected member loss, but \textit{only} if the ensemble output is actually produced by an arithmetic mean. 

However, it can not be assumed from the outset that the arithmetic mean is the best ensemble combiner. Indeed, for the cross-entropy loss, \citeauthor{abe} proceed to note that the Jensen gap corresponds to a form that is "not immediately recognizable". Although they do find an interpretation of it, it is still necessarily dependent on the outcome $Y$ 
\marginnote{
	In fact, for the case of cross-entropy, \cite{wood23} show that the ambiguity term is still nonnegative, i.e. that the arithmetic mean combiner does not hurt performance. 
}.
As illustrated in \ref{sec:bregman-divergences} on Bregman divergences, 
it seems reasonable to define the ensemble combiner in accordance to the Bregman divergence, i.e. to be the \textit{dual} expectation $\mathcal{E}_{\Theta}\left[ q_{\Theta} \right]$. 
% TODO actually talk about these combiners somewhere
Non-negativity is then easily shown since in that case ambiguity-effect reduces to ambiguity (see \ref{above})
$$
\Breg{\bar{q}}{q} = \mathbb{E}_{}\left[ \Breg{Y}{\bar{q}} - \Breg{Y}{q} \right]
\hspace{1em} \text{for $\bar{q} = \mathcal{E}_\Theta\left[q_\Theta\right]$}
$$ and the value of any Bregman divergence is always non-negative. Further, ambiguity is now independent of the outcome.

This shows that for any Bregman divergence, ensembling using the combiner implied by the divergence can not hurt performance. Second, we obtain an intuitive measure of ensemble improvement. Third, this ensemble improvement appears in an exact decomposition of the ensemble generalisation error.
% TODO can potentially shorten this
% TODO wrap in corollary?




% TODO wood23 "MC-dropout"
\end{document} 