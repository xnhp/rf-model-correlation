\documentclass[../main.tex]{subfiles}
\begin{document}

\section{The Diversity-Effect Decomposition}
\label{sec:bias-variance-diversity-decomp}

% TODO write somewhere that bayes error can also be considered a variance (see Adlam22 eq (5)
% TODO cite 
%- krog_NeuralNetworkEnsembles
% - wood23
% TODO clarify that were omitting E_(X,Y) herere
% TODO in other places, randomness of models is within RV D, here we use Theta
\paragraph{Generalising Ambiguity} In \cref{sec:bias-variance-effects}, we have derived a generalised bias-variance decomposition by considering the effect on the loss of using a random variable instead of its non-random centroid. 
For the variance, this would be the expected difference in loss between using a model dependent on the training input $D$ and the expected model, where the expectation is over $D$. 
Consider now an ensemble of learners, in which each learner is constructed according to a random parameter $\Theta$. Similar to bias and variance, we may consider the distribution of models over $\Theta$ around a central model $\bar{q}$ that is non-random with respect to $\Theta$. 
The expected \textit{effect} of using the central model instead of some single member is expressed as follows.
$$
\mathbb{E}_{\Theta} \left[
\LE{\bar{q}}{q_\Theta}
\right]
=
\mathbb{E}_{\Theta}\left[   \ell(Y, q_{\Theta}) -  \ell(Y, \bar{q})) \right] \approx \Mavg \ell(Y, q_i) - \ell(Y, \bar{q})
$$


Using the same strategy as for the bias-variance-effect decomposition of \cref{thm:bias-variance-effect}, we can use this to formulate a decomposition of the generalisation error with respect to the central model.

\marginfigfromsource{fig:ambiguity-spread}{symlinks/illustrations/ambiguity/rf-ambiguity}

% TODO need to somewhere make clear that we write without noise everywhere

\begin{theorem} \label{thm:ambig-effect-decomp} (Ambiguity-Effect decomposition \cite{wood_UnifiedTheoryDiversity_2023}) For any loss function $\ell$, target label $Y$, ensemble members $q_{1}, \dots, q_{M}$ with combiner $\bar{q}$
\begin{align*}
\ell(Y, \bar{q}) &= \Mavg \ell(Y, q_{i}) -
\underbrace{\left(\Mavg \ell(Y, q_{i}) - \ell(Y, \bar{q})\right)}_{\text{Ambiguity-Effect / Ensemble Improvement}} \\
&= \Mavg \ell(Y, q_i) - \Mavg \LE{\bar{q}}{q_i}
\end{align*}
% TODO use LE notation
\end{theorem}
\marginnote{
Similar to variance, ambiguity and ambiguity-effect are measures of spread. 
Variance measures the spread of training error across models trained with different draws of the training dataset $D$ around a model that is a centroid with respect to the distribution of $D$. Similarly, ambiguity measures the spread of individual member model errors around a model that is centroid with respect to the distribution of $\Theta$, namely the combiner $\bar{q}$. In case of squared loss, this is indeed the statistical variance around the arithmetic mean. For other losses, this is a different quantity.
}
If the ensemble combiner $\bar{q}$ is chosen to be the central model (\cf \ref{def:ensemble-combiner}), this is a decomposition of the ensemble generalisation error and a generalisation of the ambiguity decomposition described in \cref{sec:bias-variance-diversity-decomp}.

% TODO show that majority vote is actually centroid to 0-1-loss (have that in touchnotes somewhere)
We can then also find an intuitive interpretation of ambiguity-effect: It is the effect of ensembling on the error. If we consider the members to be constructed according to a parameter $\Theta$, a reasonable measure of the member performance is its loss in expectation over the parameter distribution: $\mathbb{E}_{\Theta}\left[ \ell(Y, q(X; \Theta))  \right] \approx \Mavg \ell(Y, q(X;\Theta_{i})$. What we gain or lose from using an ensemble $\bar{q}$ over just a single member model is exactly measured by ambiguity-effect. Due to this, this quantity is also known as \textit{ensemble improvement} \cite{theisen_WhenAreEnsembles_2023,abe_BestDeepEnsembles_2022}.

% TODO some sentence on how this was defined as ambiguity-effect in krogh or sth


% TODO wood23 motivate this from perspective that ambig-eff is special case of bias-variance decomp

% \begin{marginfigure} \label{fig:ambiguity-spread}
%     \includegraphics[width=\textwidth]{symlinks/illustrations/ambiguity/rf-ambiguity.png}
%     \input{symlinks/illustrations/ambiguity/rf-ambiguity.tex}
% \end{marginfigure}


\paragraph{Diversity-Effect} The ambiguity decomposition divides the ensemble error into the average member error and the variance among members. How does this relate to the well-known bias-variance decomposition? Indeed, nothing prevents us from applying the bias-variance decomposition of \cref{thm:bias-variance-effect} to the average member error, resulting in a decomposition into \textit{average} bias, \textit{average} variance and expected ambiguity, which is also referred to as \textit{diversity} \cite{wood_UnifiedTheoryDiversity_2023}.
In summary, the decomposition is given in the following theorem. Note that it holds for \textit{any} loss function. 

\begin{marginfigure}
    \includegraphics[width=\textwidth]{symlinks/zero-one-plots/bvd-decomps/plot_bvd_standard_rf/mnist_subset/bvd-standard_rf.png}
    \label{fig:spambase-standard-rf-classifier-bvd}
    % TODO legend
    \caption{
        Components of the diversity-effect decomposition by number of trees in a standard Random Forest ensemble trained on \textit{mnist-subset}. \avgBiasCircle~Average bias and \avgVarianceCircle~average variance stay (almost) constant with increasing number of trees, while \diversityCircle~diversity increases. The ensemble error is the difference between the average member error (which is the sum of average bias and average variance) and diversity.
    }
\end{marginfigure}

\begin{theorem} (Bias-Variance-Diversity-Effect decomposition \cite{wood_UnifiedTheoryDiversity_2023})
    \label{thm:bias-variance-diversity-effect-decomp}
    % TODO repeat what star, bar, Y is
\begin{align*}
\mathbb{E}_{}\left[ \ell(y, \bar{q} ) \right]  &=  
\overbrace{
\mathbb{E}_{}\left[ 
    \Mavg \ell(y, q_{i} ) 
\right]
}^{\text{avg. member error}}
-  
    \overbrace{
\mathbb{E}_{}\left[ 
        \Mavg \LE{\bar{q}}{q_{i}}  
    \right]  
    }^{\text{ambiguity-effect}} \\
&= 
    \underbrace{
\mathbb{E}_{}\left[ 
        \Mavg \LE{q_{i}^\star}{y^\star }
     \right]  
    }_{\text{avg. bias-effect}}
+
    \underbrace{
\mathbb{E}_{}\left[ 
        \Mavg \LE{q_{i}^\star}{q_{i}}  
\right]
    }_{\text{avg. variance-effect}}
-
    \underbrace{
 \mathbb{E}_{}\left[ 
        \Mavg \LE{\bar{q}}{q_{i}} 
\right] 
    }_{\text{diversity-effect}}
% old stuff
    % \mathbb{E}_{(X,Y),D}\left[ \ell(Y, \bar{q}) \right] &= 
    % \underbrace{ \mathbb{E}_{Y}\left[ \ell(y, y^\star) \right] }_{\text{noise}} \\
    % &+
    % % \mathbb{E}_{\Theta, Y}\left[ L(Y, q_{\Theta}^\star) - L(Y, y^\star)\right] % avg bias-effect
    % \underbrace{
    %     \frac{1}{M} \sum_{i=1}^M \mathbb{E}_Y\left[\ell\left(Y, q_i^*\right)-\ell\left(Y, Y^*\right)\right]
    % }_{\overline{\text{bias-effect}}} \\
    % &+
    % \underbrace{
    %     \frac{1}{M} \sum_{i=1}^M \mathbb{E}_D\left[\mathbb{E}_Y\left[\ell\left(Y, q_i\right)-\ell\left(Y, q_i^*\right)\right]\right]
    % }_{\overline{\text{variance-effect}}} \\
    % % \mathbb{E}_{\Theta, D, Y}\left[ L(Y, q_{\Theta}) - L(Y, q_{\Theta}^\star) \right]  % avg variance-effect
    % &- % minus!!
    % % \mathbb{E}_{D, Y, \Theta}\left[ L(Y, q_{\Theta} - L(Y, \bar{q})) \right] % avg div-effect
    % \underbrace{
    % \mathbb{E}_D\left[\mathbb{E}_Y\left[\frac{1}{M} \sum_{i=1}^M\left[\ell\left(Y, q_i\right)-\ell(Y, \bar{q})\right]\right]\right]
    % }_{\text{diversity-effect}}
\end{align*}
% TODO use LE notation
\end{theorem}

\paragraph{Unchanged bias and reduction in variance}
\label{sec:unchanged-bias}
% TODO not really seeing this in my experiments??

In \cref{sec:ensemble-learning-motivation} we gave arguments for how in specific cases, the ensemble bias equals the bias of any member. We now have the tools to show this in a more general manner \cite{wood_UnifiedTheoryDiversity_2023}.
For the ensemble bias, application of the ambiguity-effect decomposition (\cf \ref{thm:ambig-effect-decomp}) to a set of centroid models $q_{i}^\star$ yields:
% TODO also use LE (loss-effect) notation in other places -- but make sure we're not confusing anything
$$
\underbrace{
\LE{y}{\bar{q}^\star} 
}_{\text{ens. bias}}
= 
\underbrace{
\Mavg \LE{y}{q_{i}^\star}
}_{\text{avg. bias}}
- 
\underbrace{
\Mavg\LE{\bar{q}^\star}{q_{i}^\star}
}_{\Delta}
$$
For the ensemble variance, application of the diversity-effect decomposition (\ref{thm:bias-variance-diversity-effect-decomp}) while substituting $y \gets \bar{q}^\star$:
$$
\underbrace{
\mathbb{E}_{D}\left[ \LE{\bar{q}^\star}{\bar{q}} \right]
}_{\text{ens. var.}}
 = 
\underbrace{
\Mavg \LE{\bar{q}^\star}{q_{i}^\star} 
}_{\Delta}
+ 
\underbrace{
\Mavg \mathbb{E}_{D}\left[ \LE{q_{i}^\star}{q_{i}} \right] 
}_{\text{avg. var.}}
- 
\underbrace{
\mathbb{E}_{D}\left[ \Mavg \LE{\bar{q}}{q_{i}} \right] 
}_{\text{diversity}}
$$
Due to \cref{thm:qstars-same} which states that in homogeneous ensembles $q_{i}^\star = q_{j}^\star = \bar{q}^\star$, we can conclude that $\Delta = 0$. 
\begin{corollary} For homogeneous ensembles, we can conclude the following:
\begin{itemize}
    \item The ensemble bias is equal to the average member bias:
$$
\Delta = 0 ~ ~ \rightarrow ~ ~ \LE{y}{\bar{q}^\star} = \Mavg \LE{y}{q_{i}^\star}
$$
\item \textit{Diversity is a component of ensemble variance}. The other component is the average member variance. In other words, \textit{ensemble variance reduction is measured exactly by diversity}. 
\item Diversity is bounded from above by the average member variance.
\end{itemize}
\end{corollary}



% TODO what about note in wood23 that \mathbb{V} and L are not necessarily the same?

% TODO diversity is expected value of ambiguity

% TOOD trying to derive a decomp for 01-loss, KohaviWolpert complain that variance should be nonnegative -- well, we have that here



\section{Diversity for Bregman Divergences} \label{sec:bregman-divergences}

% TODO super interesting: bregman information, is just sample variance for squared loss, mutual information for KL-divergence, also interpretable for Itakura-Saito -- so here it sort of also appears in the purity measures
% https://jmlr.csail.mit.edu/papers/volume6/banerjee05b/banerjee05b.pdf
% also note their application of Jense's inequality: the difference between the two sides is exactly equal to the bregman information
% -- could this be the ensemble improvement?!??!
% ah yes, okay, we can already sort of read that between the lines from what we already have

% TODO somewhere: table with common loss funcitons, their generators, combiners

% TODO review on breggies: applications e.g. in generative modelling, GANs? 
% chatbot says:
% Good question. One field where maximizing the generator function can be important is in generative modeling/unsupervised learning tasks.
% Specifically, in generative adversarial networks (GANs):
% - GANs aim to learn a generative model G that produces samples resembling the real data distribution Pdata.
% - G competes against a discriminative model D whose goal is to distinguish real from generated samples. 
% - D's training objective is to maximize the probability of assigning the correct label (real vs fake) to both real and generated samples.
% - In other words, D aims to maximize the divergence metric (usually JS divergence) between Pdata and Pg (the generated distribution).
% - Maximizing this divergence encourages D to become better at discrimination, which provides stronger feedback to help G improve.
% So in GAN training, the discriminator D plays the role of maximizing the generator function (divergence metric), in order to facilitate learning the best generative model G. Maximizing the divergence is important for the adversarial process that improves both models.
% Other areas like domain adaptation may also involve maximizing divergences between distributions to learn robust features/mappings across domains.

Note that bias-effect, variance-effect and ambiguity-effect are all of the form $\mathbb{E}_{}\left[ \LE{\circ}{\square} \right]$ and depend directly on the target label $Y$. 
With variance-effect, we capture the {effect} of variations between different training datasets on the prediction error.
With ambiguity-effect, we have captured the effect of variations between the different member models on the prediction error.
We have already seen that for the squared error, the variance-effect coincides with familiar notion of "statistical" variance between the predictions.
We will now argue that for Bregman divergences (\cf \ref{def:bregman-divergence}), the loss-effect terms reduce to the loss between the two objects.
The class of Bregman divergences covers many widely used loss functions and thus allows us to formulate a unified bias-variance-decomposition. 


% TODO highlight how we are losing one expectation, and which one that is
% TODO motivate more on why centroids

% #TODO its not really harder, just need to understand Thm 0.1 of Pfau13 (and maybe mention that this is essentially a statement about effect terms, but wasnt explicitly considered)
% if we want to make the point about centroids etc clearer, we already give the basis for thm0.1, so might as well do that.
% #todo also provide thm0.1 (needed for bias it seems, but first check that they really are not the same)
% the following is required for variance and ambiguity
\begin{lemma} \label{thm:bregman-collapse-bias} (\cite{pfau_GeneralizedBiasVarianceDecomposition_}, Theorem 0.1b)
    Let $q$ be a function of a random variable $Z$ and independent of $Y$. For $y^\star = \mathbb{E}_{Y}\left[ Y \right]$, i.e. the right Bregman centroid w.r.t. $Y$, it holds that
$$
\mathbb{E}_{Z}\left[ \Breg{y^\star}{q} \right]  = \mathbb{E}_{Z,Y}\left[ \Breg{Y}{q} - \Breg{Y}{y^\star} \right] 
$$
% Let the generator $\phi: \mathcal{S} \to \mathbb{R}$ be a strictly convex, differentiable function. Let $Y$ be a random variable on $\mathcal{S}$. Then, for any $q \in \mathcal{S}$, it holds that
% % #todo will need to define bregman centroid. what kind of centroid do i need for variance lemma?
% $$
% \Breg{y^\star}{q} = \mathbb{E}_{}\left[ \Breg{Y}{q}  - \Breg{Y}{y^\star} \right]
% $$
% if $y^\star$ is the right Bregman centroid.
\end{lemma}

Using $q \gets q^\star$ shows that bias-effect collapses to bias for Bregman divergences: 
$$\Breg{y^\star}{q^\star} = \mathbb{E}_{Y}\left[ \Breg{Y}{q^\star} - \Breg{Y}{y^\star} \right]$$

\begin{lemma} \label{thm:bregman-collapse-variances}$\star$ (Generalised from \cite{wood_UnifiedTheoryDiversity_2023})
Let $q$ be a function of random variable $Z$ and independent of $Y$. For $q^\star = \mathcal{E}_{Z}\left[ q \right]$, i.e. the left Bregman centroid w.r.t. $Z$, it holds that
$$
\mathbb{E}_{Z}\left[ \Breg{q^\star}{q} \right] 
 = \mathbb{E}_{Z,Y}\left[ \Breg{Y}{q} - \Breg{Y}{q^\star} \right] 
$$
\end{lemma}
\begin{proof}
The proof is given in \cref{proof:bregman-collapse-variances}.
\end{proof}
% TODO note that this is dual expectation, and repeat lemmas from paper indicating that the dual expectations are indeed centroids
% TODO more on how this is all about variation around centroids and what the centroids are in example of variance and ambiguity

% TODO highlight somewhere that this is a slightly different way of showing the decomp -- was previously shown by directly showing bias-variance decomp for it; here we start with effect decomp which trivially holds and then apply bregman collapse trick (which is inherent in the other proof too, but certainly not as clear)

% TODO write D, Theta in both equations, needs to be clearer I guess
\cref{thm:bregman-collapse-variances} shows that variance- and ambiguity-effect collapse to variance and ambiguity. The variance (in the bias-variance sense) is the variance of the estimates $q_D$ dependent on $D$ with respect to different realisations of $D$ around its $D$-centroid. 
$$
\text{For $q^\star = \mathcal{E}_D\left[q_D\right]$:}\hspace{1em}
\Breg{q^\star}{q} = \mathbb{E}_{}\left[ \Breg{Y}{q} - \Breg{Y}{q^\star} \right]
$$
Ambiguity/Diversity is the variance of the estimates $q_\Theta$ dependent on $\Theta$ with respect to different realisations of $\Theta$ around its centroid.
$$
\text{For $\bar{q} = \mathcal{E}_\Theta\left[q_\Theta\right]$:}\hspace{1em}
\Breg{\bar{q}}{q} = \mathbb{E}_{Y}\left[ \Breg{Y}{q} - \Breg{Y}{\bar{q}} \right]
$$
% TOD do these have a geometric intuition?

\begin{figure}
$$
\begin{array}{lll}
\hline \text { Divergence } & \text { Combiner } & \text { Name } \\
\hline \text { Squared loss } & \frac{1}{M} \sum_{i=1}^M q_i & \text { Arithmetic mean } \\
\text { Poisson regression loss } & \prod_{i=1}^M q_i^{\frac{1}{M}} & \text { Geometric mean } \\
\text { KL-divergence } & Z^{-1} \prod_{i=1}^M\left(\mathbf{q}_i^{(c)}\right)^{\frac{1}{M}} & \text { Normalised geometric mean } \\
\text { Itakura-Saito loss } & 1 /\left(\frac{1}{M} \sum_{i=1}^M \frac{1}{q_i}\right) & \text { Harmonic mean } \\
\hline
\end{array}
$$
\caption{Examples for the combiner $\mathcal{E}_\Theta \left[q_\Theta\right]$ implied by a Bregman divergence \cite{wood_UnifiedTheoryDiversity_2023}.}
\end{figure}

This yields a generalised bias-variance-diversity decomposition for Bregman divergences as a special case of the corresponding effect decomposition.

% TODO notation, (re)define symbols
% TODO clarify: expectation over X
% TODO clarify: q depends on X (and D?), q* depends only on X
% #todo writing E_Y|X here, reconcile with other things
% \begin{theorem} (\textit{Bias-variance-decomposition for Bregman divergences}) \label{thm:bregman-bias-variance-decomp}
% $$
% \mathbb{E}_{(X,Y), D}\left[ \Breg{Y}{q} \right] 
% = \underbrace{\mathbb{E}_{Y|X}\left[ \Breg{Y}{y^\star} \right]   }_{\text{noise}}
% + \underbrace{\Breg{y^\star}{q^\star} }_{\text{bias}}
% + \underbrace{\mathbb{E}_{D}\left[ \Breg{q^\star}{q} \right] }_{\text{variance}}
% $$
% % #todo make sure the order arguments is as requried by the lemmas
% \end{theorem}

\begin{theorem} (Bias-Variance-Diversity decomposition for Bregman divergences)
    \label{thm:bregman-diversity-decomp}
    % TODO define qstar and qbar in terms of (dual) expectations (left/right centroids)
    % TODO define Ybar -- should this be Ystar?
\begin{align*}
\mathbb{E}_{(X,Y), D}\left[ \Breg{Y}{q} \right]  
&= \underbrace{\mathbb{E}_{Y|X}\left[ \Breg{Y}{y^\star} \right]   }_{\text{noise}} \\
&+
\underbrace{
    \frac{1}{M} \sum_{i=1}^M B_\phi\left(y^\star, q_i^\star \right)
}_{\overline{\text{bias}}} \\
&+
\underbrace{
    \frac{1}{M} \sum_{i=1}^M \mathbb{E}_D\left[B_\phi\left(q_i^\star, q_i\right)\right]
}_{\overline{\text{variance}}} \\
&-
\underbrace{
    \mathbb{E}_D\left[\frac{1}{M} \sum_{i=1}^M B_\phi\left(\bar{q}, q_i\right)\right]
}_{\text{diversity}}
\end{align*}
\end{theorem}

% TODO describe terms
% TODO give a plot here how avg-bias, avg-variance do not change but diversity improves

% TODO note on how we require special ensemble combiner according to dual expectation for this to hold
% give table for combiners for squared-error KL-divergence etc

% !! TODO primal averaging can *hurt* performance, cf. adlam22
% TODO variance reduction cf adlam22

\paragraph{Ensemble improvement for Bregman Divergences}

In section \ref{sec:ensemble-learning-motivation}, we have used Jensen's inequality to show that the ensemble improvement is non-negative for some cases.
$$
\mathbb{E}_{{\Theta}}\left[ \ell (Y, q_{\Theta}(X)) \right]  -
\ell(Y, \mathbb{E}_{\Theta}\left[ q_{\Theta}(X) \right]) \geq 0
$$
It is evident that the Jensen gap is but a special case of ambiguity-effect (\cf \ref{thm:ambig-effect-decomp}) for $\bar{q} \defeq \Mavg q_{i}$ and convex loss functions. 
This shows that the ensemble loss is always smaller-equal than the expected member loss, but \textit{only} if the ensemble output is actually produced by an arithmetic mean. 

However, it can not be assumed from the outset that the arithmetic mean is the best ensemble combiner. Indeed, for the cross-entropy loss, \citeauthor{abe_PathologiesPredictiveDiversity_2023} \cite{abe_PathologiesPredictiveDiversity_2023} proceed to note that the Jensen gap corresponds to a form that is "not immediately recognizable". Although they do find an interpretation of it, it is still necessarily dependent on the outcome $Y$ 
\marginnote{
	In fact, for the case of cross-entropy, \cite{wood_UnifiedTheoryDiversity_2023} show that the ambiguity term is still nonnegative, i.e. that the arithmetic mean combiner does not hurt performance. 
}.
It seems reasonable to define the ensemble combiner in accordance to the Bregman divergence, i.e. to be the \textit{dual} expectation $\mathcal{E}_{\Theta}\left[ q_{\Theta} \right]$. 
% TODO actually talk about these combiners somewhere
Non-negativity is then easily shown since in that case ambiguity-effect reduces to ambiguity.
$$
\Breg{\bar{q}}{q} = \mathbb{E}_{}\left[ \Breg{Y}{q} - \Breg{Y}{\bar{q}} \right]
\hspace{1em} \text{for $\bar{q} = \mathcal{E}_\Theta\left[q_\Theta\right]$}
$$ and the value of any Bregman divergence is always non-negative. Further, ambiguity is now independent of the outcome $Y$.

\begin{corollary} ~ ~ 
    \label{cor:bregman-divergences}
    \begin{itemize}
        \item For any Bregman divergence, ensembling using the combiner implied by the divergence can not hurt performance.
        \item The diversity is an intuitive measure of ensemble improvement.
        \item This ensemble improvement (diversity) appears in an exact decomposition of the ensemble generalisation error.
    % TODO feel like theres more
    \end{itemize}
\end{corollary}


% TODO wood23 "MC-dropout"
\end{document} 